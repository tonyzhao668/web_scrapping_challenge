{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# import requests\n",
    "# import pymongo\n",
    "from splinter import Browser\n",
    "import pandas as pd\n",
    "import time\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import datetime\n",
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of page to be scraped\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "url2 = 'https://data-class-jpl-space.s3.amazonaws.com/JPL_Space/index.html'\n",
    "url3 = 'https://space-facts.com/mars/'\n",
    "url4 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 89.0.4389\n",
      "[WDM] - Get LATEST driver version for 89.0.4389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [C:\\Users\\User\\.wdm\\drivers\\chromedriver\\win32\\89.0.4389.23\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "#prepared for run splinter, initiation\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visite the required url\n",
    "browser.visit(url)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Scrape page into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the news' title and paragraph\n",
    "news = []\n",
    "\n",
    "threads = soup.find_all('li', class_='slide')\n",
    "\n",
    "for thread in threads:\n",
    "    div = thread.find('div', class_= 'list_text')\n",
    "    temp = {}\n",
    "    try: \n",
    "        dt = div.find('div', class_= 'list_date').text\n",
    "        d = parse(dt).strftime(\"%y-%m-%d\")\n",
    "    \n",
    "        if d >= '21-03-05':\n",
    "            p = div.find('div', class_= 'article_teaser_body').text\n",
    "            if div.a.text:\n",
    "                t = div.a.text\n",
    "                if t.split(\"-\"):\n",
    "                    i = t.split(\"-\")\n",
    "                    t = i[0]\n",
    "            if (d and t and p):\n",
    "                temp.update({'date': d, 'title': t, 'paragraph': p})\n",
    "                news.append(temp)\n",
    "                \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n in news:\n",
    "#     print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visite the required url2\n",
    "browser.visit(url2)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Scrape page into Soup\n",
    "html2 = browser.html\n",
    "soup = bs(html2, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the featured image and its url\n",
    "images = []\n",
    "\n",
    "di = soup.find('div', class_ = 'col-md-12 mt-5')\n",
    "\n",
    "locates = di.find_all('div', class_='col-md-4')\n",
    "\n",
    "for loc in locates:\n",
    "    temp = {}\n",
    "    try: \n",
    "        img = loc.img\n",
    "        img_url = img['src'] \n",
    "        im = img_url.split(\".\")\n",
    "        if (im[-1]) == \"jpg\":\n",
    "            temp.update({\"url\": img_url})\n",
    "            images.append(temp)\n",
    "            #print(img_url)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>items</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Equatorial Diameter:</td>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Polar Diameter:</td>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mass:</td>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Moons:</td>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Orbit Distance:</td>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  items                           data\n",
       "0  Equatorial Diameter:                       6,792 km\n",
       "1       Polar Diameter:                       6,752 km\n",
       "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
       "3                Moons:            2 (Phobos & Deimos)\n",
       "4       Orbit Distance:       227,943,824 km (1.38 AU)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Mars fact table \n",
    "tables = pd.read_html(url3)\n",
    "\n",
    "df = tables[0]\n",
    "df.columns = ['items', 'data']\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tal = df.to_html(index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visite the required url4\n",
    "browser.visit(url4)\n",
    "time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Scrape page into Soup\n",
    "html4 = browser.html\n",
    "soup = bs(html4, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build mars hemisphere title list\n",
    "titles = []\n",
    "\n",
    "dv = soup.find('div', class_ = 'collapsible results')\n",
    "\n",
    "los = dv.find_all('div', class_='item')\n",
    "\n",
    "for lo in los:\n",
    "    try: \n",
    "        h = lo.find('div', class_ = 'description')\n",
    "        title = h.a.h3.text\n",
    "        if (title.split(' ')[-1]) == 'Enhanced':\n",
    "            titles.append(title)\n",
    "        \n",
    "    except AttributeError as e:\n",
    "        print(e)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a short title list (tiles without 'hemisphere and in low case')\n",
    "short_t = []\n",
    "for i in range(4):\n",
    "    t = titles[i].split(' ')\n",
    "    short_t.append((t[0] + '_' + t[-1]).lower())\n",
    "    if i > 1:\n",
    "        short_t[i] = ((t[0] + '_' + t[1] +'_'+ t[-1]).lower())\n",
    "    \n",
    "#short_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build dict for the images' titles and urls (urls) for sample images with smaller size, for easy loading\n",
    "url_list =[]\n",
    "u = 'https://astropedia.astrogeology.usgs.gov/download/Mars/Viking/'\n",
    "for j in range(4):\n",
    "    temp = {}\n",
    "    urlo = u + short_t[j]+'.'+'tif'\n",
    "    urls = urlo + '/full.jpg'\n",
    "    temp.update({'title':titles[j], 'href': urls})\n",
    "    url_list.append(temp)\n",
    "#url_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pythondata] *",
   "language": "python",
   "name": "conda-env-pythondata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
